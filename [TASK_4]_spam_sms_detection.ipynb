{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "importing dependencies"
      ],
      "metadata": {
        "id": "7P1BZgNvb429"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "2aE04iEFyQuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading datasets"
      ],
      "metadata": {
        "id": "pL2jBi9st98G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('spam.csv', encoding='latin-1')"
      ],
      "metadata": {
        "id": "8WavsRVBt9RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())\n",
        "print(data.tail())\n",
        "print(data.describe())\n",
        "print(data.isnull().sum)\n",
        "print(data.count())\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGsRkVPhjEUG",
        "outputId": "0ef47197-4e20-4342-b0d0-8724672c96a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n",
            "        v1                                                 v2 Unnamed: 2  \\\n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
            "5568   ham              Will ÃŒ_ b going to esplanade fr home?        NaN   \n",
            "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
            "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
            "5571   ham                         Rofl. Its true to its name        NaN   \n",
            "\n",
            "     Unnamed: 3 Unnamed: 4  \n",
            "5567        NaN        NaN  \n",
            "5568        NaN        NaN  \n",
            "5569        NaN        NaN  \n",
            "5570        NaN        NaN  \n",
            "5571        NaN        NaN  \n",
            "          v1                      v2  \\\n",
            "count   5572                    5572   \n",
            "unique     2                    5169   \n",
            "top      ham  Sorry, I'll call later   \n",
            "freq    4825                      30   \n",
            "\n",
            "                                               Unnamed: 2  \\\n",
            "count                                                  50   \n",
            "unique                                                 43   \n",
            "top      bt not his girlfrnd... G o o d n i g h t . . .@\"   \n",
            "freq                                                    3   \n",
            "\n",
            "                   Unnamed: 3 Unnamed: 4  \n",
            "count                      12          6  \n",
            "unique                     10          5  \n",
            "top      MK17 92H. 450Ppw 16\"    GNT:-)\"  \n",
            "freq                        2          2  \n",
            "<bound method DataFrame.sum of          v1     v2  Unnamed: 2  Unnamed: 3  Unnamed: 4\n",
            "0     False  False        True        True        True\n",
            "1     False  False        True        True        True\n",
            "2     False  False        True        True        True\n",
            "3     False  False        True        True        True\n",
            "4     False  False        True        True        True\n",
            "...     ...    ...         ...         ...         ...\n",
            "5567  False  False        True        True        True\n",
            "5568  False  False        True        True        True\n",
            "5569  False  False        True        True        True\n",
            "5570  False  False        True        True        True\n",
            "5571  False  False        True        True        True\n",
            "\n",
            "[5572 rows x 5 columns]>\n",
            "v1            5572\n",
            "v2            5572\n",
            "Unnamed: 2      50\n",
            "Unnamed: 3      12\n",
            "Unnamed: 4       6\n",
            "dtype: int64\n",
            "(5572, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns={\"v1\": \"label\", \"v2\": \"message\"})\n",
        "data = data[[\"label\", \"message\"]]\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "YKJTMqvtn2Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzvssF60pt6N",
        "outputId": "a677d8d3-ab84-4b07-a496-353d357a974b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    0.865937\n",
            "1    0.134063\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalising data"
      ],
      "metadata": {
        "id": "T41GHHsYuiCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "eYkJxP0DiCDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf-idf\n"
      ],
      "metadata": {
        "id": "EszHNeyGun7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "ncC7t00Cupnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ],
      "metadata": {
        "id": "niODXvsNil2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_model = LogisticRegression(random_state=42)\n",
        "logistic_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_logistic = logistic_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_logistic):.2f}\")\n",
        "print(classification_report(y_test, y_pred_logistic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POKU0qt4ioEi",
        "outputId": "5f39b8f5-41ea-4d34-e56d-5776cfe9b18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "Accuracy: 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       965\n",
            "           1       0.97      0.71      0.82       150\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.96      0.85      0.90      1115\n",
            "weighted avg       0.96      0.96      0.95      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive bayes"
      ],
      "metadata": {
        "id": "b8ZAOmXOisCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Naive Bayes\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.2f}\")\n",
        "print(classification_report(y_test, y_pred_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYFM0mboitiI",
        "outputId": "d88374d6-59b9-4356-b318-217116b3206b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes\n",
            "Accuracy: 0.97\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98       965\n",
            "           1       1.00      0.79      0.88       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.98      0.90      0.93      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "support vector machine"
      ],
      "metadata": {
        "id": "XiO0g-b3pLq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Support Vector Machine\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}\")\n",
        "print(classification_report(y_test, y_pred_svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWKHEZVepMwf",
        "outputId": "92d36f41-f8e3-4048-ac38-ff1801627d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machine\n",
            "Accuracy: 0.98\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       965\n",
            "           1       0.97      0.87      0.92       150\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.98      0.93      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}